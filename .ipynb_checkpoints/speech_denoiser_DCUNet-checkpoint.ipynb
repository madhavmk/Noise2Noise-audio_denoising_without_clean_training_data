{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9oX5DLS3COS5"
   },
   "source": [
    "# Speech Denoising without Clean Training Data: a Noise2Noise Approach #\n",
    "### INTERSPEECH 2021 ###\n",
    "### Madhav Mahesh Kashyap, Anuj Tambwekar, Krishnamoorthy Manohara, S Natarajan ###\n",
    "### Department of Computer Science and Engineering, PES University, India ###\n",
    "\n",
    "<br>\n",
    "\n",
    "[Link to Published Paper](https://www.isca-speech.org/archive/pdfs/interspeech_2021/kashyap21_interspeech.pdf)\n",
    "<br>\n",
    "[Link to Conference Proceedings](https://www.isca-speech.org/archive/interspeech_2021/kashyap21_interspeech.html)\n",
    "<br><br>\n",
    "\n",
    "```\n",
    "@inproceedings{kashyap21_interspeech,\r\n",
    "  author={Madhav Mahesh Kashyap and Anuj Tambwekar and Krishnamoorthy Manohara and S. Natarajan},\r\n",
    "  title={{Speech Denoising Without Clean Training Data: A Noise2Noise Approach}},\r\n",
    "  year=2021,\r\n",
    "  booktitle={Proc. Interspeech 2021},\r\n",
    "  pages={2716--2720},\r\n",
    "  doi={10.21437/Interspeech.2021-1130}\r\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and Constants ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "\n",
    "N2N = \"Noise2Noise\"\n",
    "N2C = \"Noise2Clean\"\n",
    "\n",
    "TRAIN_INPUT_DIR, TRAIN_TARGET_DIR = None, None\n",
    "\n",
    "TEST_NOISY_DIR, TEST_CLEAN_DIR = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paper Preview ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"static\\interspeech_conference_paper.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x211698bd810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = Path(\"static\").joinpath(\"interspeech_conference_paper.pdf\")\n",
    "display(IFrame(path, width=1000, height=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter the Noise type the model should train/infer. The test and train dataset must already be generated beforehand (See README for instructions). ####\n",
    "```\n",
    "white : additive_gaussian_noise\n",
    "0     : air_conditioner\n",
    "1     : car_horn\n",
    "2     : children_playing\n",
    "3     : dog_bark\n",
    "4     : drilling\n",
    "5     : engine_idling\n",
    "6     : gun_shot\n",
    "7     : jackhammer\n",
    "8     : siren\n",
    "9     : street_music\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the Noise type the model should train/infer : -->  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have inputted the UrbanSound8K Noise type : -->   0   <--\n"
     ]
    }
   ],
   "source": [
    "noise_class = (\n",
    "    input(\"Enter the Noise type the model should train/infer : --> \").strip().lower()\n",
    ")\n",
    "\n",
    "match noise_class:\n",
    "    case \"white\":\n",
    "        print(f\"You have inputted the Synthetic Noise type : --> {noise_class:^9} <--\")\n",
    "    case _ if noise_class.isdigit() and int(noise_class) in range(0, 10):\n",
    "        print(\n",
    "            f\"You have inputted the UrbanSound8K Noise type : --> {noise_class:^5} <--\"\n",
    "        )\n",
    "    case _:\n",
    "        raise ValueError(\"Invalid Noise type inputted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter the Training type to employ; either **'Noise2Noise'**(ours) or **'Noise2Clean'**(conventional) ####\n",
    "```\n",
    "0     : Noise2Noise\n",
    "1     : Noise2Clean\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the Training type to employ : -->  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have inputted the UrbanSound8K Noise type : -->   Noise2Noise   <--\n"
     ]
    }
   ],
   "source": [
    "training_type = input(\"Enter the Training type to employ : --> \").strip()\n",
    "\n",
    "match training_type:\n",
    "    case \"0\":\n",
    "        training_type = N2N\n",
    "    case \"1\":\n",
    "        training_type = N2C\n",
    "    case _:\n",
    "        raise ValueError(\"Invalid Training type inputted!\")\n",
    "\n",
    "print(f\"You have inputted the UrbanSound8K Noise type : --> {training_type:^15} <--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Traning and Training directories:\n",
      "Training target dir:\tDatasets/US_Class0_Train_Output\n",
      "Training input dir:\tDatasets/US_Class0_Train_Input\n",
      "Test noisy dir:\t\tDatasets/US_Class0_Test_Input\n",
      "Test clean dir:\t\tDatasets/clean_testset_wav\n"
     ]
    }
   ],
   "source": [
    "if noise_class == \"white\":\n",
    "    if training_type == N2N:\n",
    "        TRAIN_TARGET_DIR = Path(\"Datasets\").joinpath(\"WhiteNoise_Train_Output\")\n",
    "    else:\n",
    "        TRAIN_TARGET_DIR = Path(\"Datasets\").joinpath(\"clean_trainset_28spk_wav\")\n",
    "\n",
    "    TRAIN_INPUT_DIR = Path(\"Datasets\").join\n",
    "    path(\"WhiteNoise_Train_Input\")\n",
    "    TEST_NOISY_DIR = Path(\"Datasets\").joinpath(\"WhiteNoise_Test_Input\")\n",
    "\n",
    "else:\n",
    "    if training_type == N2N:\n",
    "        TRAIN_TARGET_DIR = Path(\"Datasets\").joinpath(\n",
    "            f\"US_Class{noise_class}_Train_Output\"\n",
    "        )\n",
    "    else:\n",
    "        TRAIN_TARGET_DIR = Path(\"Datasets\").joinpath(\"clean_trainset_28spk_wav\")\n",
    "\n",
    "    TRAIN_INPUT_DIR = Path(\"Datasets\").joinpath(f\"US_Class{noise_class}_Train_Input\")\n",
    "    TEST_NOISY_DIR = Path(\"Datasets\").joinpath(\n",
    "        \"US_Class\" + str(noise_class) + \"_Test_Input\"\n",
    "    )\n",
    "\n",
    "TEST_CLEAN_DIR = Path(\"Datasets\").joinpath(\"clean_testset_wav\")\n",
    "\n",
    "print(\n",
    "    f\"Training and Testing directories listed below:\\n\"\n",
    "    f\"Training target dir:\\t{TRAIN_TARGET_DIR.parent}/{TRAIN_TARGET_DIR.name}\\n\"\n",
    "    f\"Training input dir:\\t{TRAIN_INPUT_DIR.parent}/{TRAIN_INPUT_DIR.name}\\n\"\n",
    "    f\"Test noisy dir:\\t\\t{TEST_NOISY_DIR.parent}/{TEST_NOISY_DIR.name}\\n\"\n",
    "    f\"Test clean dir:\\t\\t{TEST_CLEAN_DIR.parent}/{TEST_CLEAN_DIR.name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = str(noise_class) + \"_\" + training_type\n",
    "os.makedirs(basepath, exist_ok=True)\n",
    "os.makedirs(basepath + \"/Weights\", exist_ok=True)\n",
    "os.makedirs(basepath + \"/Samples\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D0bWtt2J5i9F"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnoise_addition_utils\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AudioMetrics\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AudioMetrics2\n",
      "File \u001b[1;32mD:\\speech_denoising_without_clean_training_data\\Noise2Noise-audio_denoising_without_clean_training_data\\noise_addition_utils.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import gc\n",
    "\n",
    "import noise_addition_utils\n",
    "\n",
    "from metrics import AudioMetrics\n",
    "from metrics import AudioMetrics2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import colors, pyplot as plt\n",
    "from pypesq import pesq\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# not everything is smooth in sklearn, to conveniently output images in colab\n",
    "# we will ignore warnings\n",
    "warnings.filterwarnings(action=\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(999)\n",
    "torch.manual_seed(999)\n",
    "\n",
    "# If running on Cuda set these 2 for determinism\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xnlgTsICOTA"
   },
   "source": [
    "### Checking whether the GPU is available ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "byUnPtQ25i9O",
    "outputId": "4b6d9f85-e5b6-4cb5-c3db-2f48321ed391"
   },
   "outputs": [],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if train_on_gpu:\n",
    "    print(\"Training on GPU.\")\n",
    "else:\n",
    "    print(\"No GPU available, training on CPU.\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if train_on_gpu else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "qacrfNwA6vw_",
    "outputId": "e1183ab5-4f39-478a-b00a-74fe5edfb511"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_N4AFJANDcBG"
   },
   "source": [
    "### Set Audio backend as Soundfile for windows and Sox for Linux ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_N4AFJANDcBG"
   },
   "outputs": [],
   "source": [
    "torchaudio.set_audio_backend(\"soundfile\")\n",
    "print(\"TorchAudio backend used:\\t{}\".format(torchaudio.get_audio_backend()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IWvlIABzCOTM"
   },
   "source": [
    "### The sampling frequency and the selected values for the Short-time Fourier transform. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ngFJtPj5i9V"
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 48000\n",
    "N_FFT = (SAMPLE_RATE * 64) // 1000\n",
    "HOP_LENGTH = (SAMPLE_RATE * 16) // 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x8suREWkb5Se"
   },
   "source": [
    "### The declaration of datasets and dataloaders ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cZ0wb9EN5i9f"
   },
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset class with audio that cuts them/paddes them to a specified length, applies a Short-tome Fourier transform,\n",
    "    normalizes and leads to a tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, noisy_files, clean_files, n_fft=64, hop_length=16):\n",
    "        super().__init__()\n",
    "        # list of files\n",
    "        self.noisy_files = sorted(noisy_files)\n",
    "        self.clean_files = sorted(clean_files)\n",
    "\n",
    "        # stft parameters\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "        self.len_ = len(self.noisy_files)\n",
    "\n",
    "        # fixed len\n",
    "        self.max_len = 165000\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_\n",
    "\n",
    "    def load_sample(self, file):\n",
    "        waveform, _ = torchaudio.load(file)\n",
    "        return waveform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load to tensors and normalization\n",
    "        x_clean = self.load_sample(self.clean_files[index])\n",
    "        x_noisy = self.load_sample(self.noisy_files[index])\n",
    "\n",
    "        # padding/cutting\n",
    "        x_clean = self._prepare_sample(x_clean)\n",
    "        x_noisy = self._prepare_sample(x_noisy)\n",
    "\n",
    "        # Short-time Fourier transform\n",
    "        x_noisy_stft = torch.stft(\n",
    "            input=x_noisy, n_fft=self.n_fft, hop_length=self.hop_length, normalized=True\n",
    "        )\n",
    "        x_clean_stft = torch.stft(\n",
    "            input=x_clean, n_fft=self.n_fft, hop_length=self.hop_length, normalized=True\n",
    "        )\n",
    "\n",
    "        return x_noisy_stft, x_clean_stft\n",
    "\n",
    "    def _prepare_sample(self, waveform):\n",
    "        waveform = waveform.numpy()\n",
    "        current_len = waveform.shape[1]\n",
    "\n",
    "        output = np.zeros((1, self.max_len), dtype=\"float32\")\n",
    "        output[0, -current_len:] = waveform[0, : self.max_len]\n",
    "        output = torch.from_numpy(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NLV2lcv5i9k"
   },
   "outputs": [],
   "source": [
    "train_input_files = sorted(list(TRAIN_INPUT_DIR.rglob(\"*.wav\")))\n",
    "train_target_files = sorted(list(TRAIN_TARGET_DIR.rglob(\"*.wav\")))\n",
    "\n",
    "test_noisy_files = sorted(list(TEST_NOISY_DIR.rglob(\"*.wav\")))\n",
    "test_clean_files = sorted(list(TEST_CLEAN_DIR.rglob(\"*.wav\")))\n",
    "\n",
    "print(\"No. of Training files:\", len(train_input_files))\n",
    "print(\"No. of Testing files:\", len(test_noisy_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7GHaKjYS5i9u"
   },
   "outputs": [],
   "source": [
    "test_dataset = SpeechDataset(test_noisy_files, test_clean_files, N_FFT, HOP_LENGTH)\n",
    "train_dataset = SpeechDataset(train_input_files, train_target_files, N_FFT, HOP_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "79nIQjht5i9y"
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# For testing purpose\n",
    "test_loader_single_unshuffled = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tHJZXeUQcsrq"
   },
   "source": [
    "### Average Test Set Metrics ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set_metrics(test_loader, model):\n",
    "    metric_names = [\"CSIG\", \"CBAK\", \"COVL\", \"PESQ\", \"SSNR\", \"STOI\"]\n",
    "    overall_metrics = [[] for i in range(len(metric_names))]\n",
    "\n",
    "    for i, (noisy, clean) in enumerate(test_loader):\n",
    "        x_est = model(noisy.to(DEVICE), is_istft=True)\n",
    "        x_est_np = x_est[0].view(-1).detach().cpu().numpy()\n",
    "        x_c_np = (\n",
    "            torch.istft(\n",
    "                torch.squeeze(clean[0], 1),\n",
    "                n_fft=N_FFT,\n",
    "                hop_length=HOP_LENGTH,\n",
    "                normalized=True,\n",
    "            )\n",
    "            .view(-1)\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "        metrics = AudioMetrics(x_c_np, x_est_np, SAMPLE_RATE)\n",
    "\n",
    "        overall_metrics[0].append(metrics.CSIG)\n",
    "        overall_metrics[1].append(metrics.CBAK)\n",
    "        overall_metrics[2].append(metrics.COVL)\n",
    "        overall_metrics[3].append(metrics.PESQ)\n",
    "        overall_metrics[4].append(metrics.SSNR)\n",
    "        overall_metrics[5].append(metrics.STOI)\n",
    "\n",
    "    metrics_dict = dict()\n",
    "    for i in range(len(metric_names)):\n",
    "        metrics_dict[metric_names[i]] = {\n",
    "            \"mean\": np.mean(overall_metrics[i]),\n",
    "            \"std_dev\": np.std(overall_metrics[i]),\n",
    "        }\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k73YEkgQCOTj"
   },
   "source": [
    "### Declaring the class layers ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Znx7QM3h5i92"
   },
   "outputs": [],
   "source": [
    "class CConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Class of complex valued convolutional layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "\n",
    "        self.real_conv = nn.Conv2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            padding=self.padding,\n",
    "            stride=self.stride,\n",
    "        )\n",
    "\n",
    "        self.im_conv = nn.Conv2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            padding=self.padding,\n",
    "            stride=self.stride,\n",
    "        )\n",
    "\n",
    "        # Glorot initialization.\n",
    "        nn.init.xavier_uniform_(self.real_conv.weight)\n",
    "        nn.init.xavier_uniform_(self.im_conv.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "\n",
    "        c_real = self.real_conv(x_real) - self.im_conv(x_im)\n",
    "        c_im = self.im_conv(x_real) + self.real_conv(x_im)\n",
    "\n",
    "        output = torch.stack([c_real, c_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GgtxJbSQ5i96"
   },
   "outputs": [],
   "source": [
    "class CConvTranspose2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Class of complex valued dilation convolutional layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        output_padding=0,\n",
    "        padding=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.output_padding = output_padding\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "\n",
    "        self.real_convt = nn.ConvTranspose2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            output_padding=self.output_padding,\n",
    "            padding=self.padding,\n",
    "            stride=self.stride,\n",
    "        )\n",
    "\n",
    "        self.im_convt = nn.ConvTranspose2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            output_padding=self.output_padding,\n",
    "            padding=self.padding,\n",
    "            stride=self.stride,\n",
    "        )\n",
    "\n",
    "        # Glorot initialization.\n",
    "        nn.init.xavier_uniform_(self.real_convt.weight)\n",
    "        nn.init.xavier_uniform_(self.im_convt.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "\n",
    "        ct_real = self.real_convt(x_real) - self.im_convt(x_im)\n",
    "        ct_im = self.im_convt(x_real) + self.real_convt(x_im)\n",
    "\n",
    "        output = torch.stack([ct_real, ct_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OJSmVrxp5i9-"
   },
   "outputs": [],
   "source": [
    "class CBatchNorm2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Class of complex valued batch normalization layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features,\n",
    "        eps=1e-05,\n",
    "        momentum=0.1,\n",
    "        affine=True,\n",
    "        track_running_stats=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "\n",
    "        self.real_b = nn.BatchNorm2d(\n",
    "            num_features=self.num_features,\n",
    "            eps=self.eps,\n",
    "            momentum=self.momentum,\n",
    "            affine=self.affine,\n",
    "            track_running_stats=self.track_running_stats,\n",
    "        )\n",
    "        self.im_b = nn.BatchNorm2d(\n",
    "            num_features=self.num_features,\n",
    "            eps=self.eps,\n",
    "            momentum=self.momentum,\n",
    "            affine=self.affine,\n",
    "            track_running_stats=self.track_running_stats,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "\n",
    "        n_real = self.real_b(x_real)\n",
    "        n_im = self.im_b(x_im)\n",
    "\n",
    "        output = torch.stack([n_real, n_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N7W37XMO5i-B"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Class of upsample block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        filter_size=(7, 5),\n",
    "        stride_size=(2, 2),\n",
    "        in_channels=1,\n",
    "        out_channels=45,\n",
    "        padding=(0, 0),\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.filter_size = filter_size\n",
    "        self.stride_size = stride_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.padding = padding\n",
    "\n",
    "        self.cconv = CConv2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.filter_size,\n",
    "            stride=self.stride_size,\n",
    "            padding=self.padding,\n",
    "        )\n",
    "\n",
    "        self.cbn = CBatchNorm2d(num_features=self.out_channels)\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        conved = self.cconv(x)\n",
    "        normed = self.cbn(conved)\n",
    "        acted = self.leaky_relu(normed)\n",
    "\n",
    "        return acted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fuugYDZs5i-G"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Class of downsample block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        filter_size=(7, 5),\n",
    "        stride_size=(2, 2),\n",
    "        in_channels=1,\n",
    "        out_channels=45,\n",
    "        output_padding=(0, 0),\n",
    "        padding=(0, 0),\n",
    "        last_layer=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.filter_size = filter_size\n",
    "        self.stride_size = stride_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.output_padding = output_padding\n",
    "        self.padding = padding\n",
    "\n",
    "        self.last_layer = last_layer\n",
    "\n",
    "        self.cconvt = CConvTranspose2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.filter_size,\n",
    "            stride=self.stride_size,\n",
    "            output_padding=self.output_padding,\n",
    "            padding=self.padding,\n",
    "        )\n",
    "\n",
    "        self.cbn = CBatchNorm2d(num_features=self.out_channels)\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        conved = self.cconvt(x)\n",
    "\n",
    "        if not self.last_layer:\n",
    "            normed = self.cbn(conved)\n",
    "            output = self.leaky_relu(normed)\n",
    "        else:\n",
    "            m_phase = conved / (torch.abs(conved) + 1e-8)\n",
    "            m_mag = torch.tanh(torch.abs(conved))\n",
    "            output = m_phase * m_mag\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3G5CqR3-COT8"
   },
   "source": [
    "### Loss function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J71ny6expQeW"
   },
   "outputs": [],
   "source": [
    "from pesq import pesq\n",
    "from scipy import interpolate\n",
    "\n",
    "\n",
    "def resample(original, old_rate, new_rate):\n",
    "    if old_rate != new_rate:\n",
    "        duration = original.shape[0] / old_rate\n",
    "        time_old = np.linspace(0, duration, original.shape[0])\n",
    "        time_new = np.linspace(\n",
    "            0, duration, int(original.shape[0] * new_rate / old_rate)\n",
    "        )\n",
    "        interpolator = interpolate.interp1d(time_old, original.T)\n",
    "        new_audio = interpolator(time_new).T\n",
    "        return new_audio\n",
    "    else:\n",
    "        return original\n",
    "\n",
    "\n",
    "def wsdr_fn(x_, y_pred_, y_true_, eps=1e-8):\n",
    "    # to time-domain waveform\n",
    "    y_true_ = torch.squeeze(y_true_, 1)\n",
    "    y_true = torch.istft(y_true_, n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True)\n",
    "    x_ = torch.squeeze(x_, 1)\n",
    "    x = torch.istft(x_, n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True)\n",
    "\n",
    "    y_pred = y_pred_.flatten(1)\n",
    "    y_true = y_true.flatten(1)\n",
    "    x = x.flatten(1)\n",
    "\n",
    "    def sdr_fn(true, pred, eps=1e-8):\n",
    "        num = torch.sum(true * pred, dim=1)\n",
    "        den = torch.norm(true, p=2, dim=1) * torch.norm(pred, p=2, dim=1)\n",
    "        return -(num / (den + eps))\n",
    "\n",
    "    # true and estimated noise\n",
    "    z_true = x - y_true\n",
    "    z_pred = x - y_pred\n",
    "\n",
    "    a = torch.sum(y_true**2, dim=1) / (\n",
    "        torch.sum(y_true**2, dim=1) + torch.sum(z_true**2, dim=1) + eps\n",
    "    )\n",
    "    wSDR = a * sdr_fn(y_true, y_pred) + (1 - a) * sdr_fn(z_true, z_pred)\n",
    "    return torch.mean(wSDR)\n",
    "\n",
    "\n",
    "wonky_samples = []\n",
    "\n",
    "\n",
    "def getMetricsonLoader(loader, net, use_net=True):\n",
    "    net.eval()\n",
    "    # Original test metrics\n",
    "    scale_factor = 32768\n",
    "    # metric_names = [\"CSIG\",\"CBAK\",\"COVL\",\"PESQ\",\"SSNR\",\"STOI\",\"SNR \"]\n",
    "    metric_names = [\"PESQ-WB\", \"PESQ-NB\", \"SNR\", \"SSNR\", \"STOI\"]\n",
    "    overall_metrics = [[] for i in range(5)]\n",
    "    for i, data in enumerate(loader):\n",
    "        if (i + 1) % 10 == 0:\n",
    "            end_str = \"\\n\"\n",
    "        else:\n",
    "            end_str = \",\"\n",
    "        # print(i,end=end_str)\n",
    "        if i in wonky_samples:\n",
    "            print(\"Something's up with this sample. Passing...\")\n",
    "        else:\n",
    "            noisy = data[0]\n",
    "            clean = data[1]\n",
    "            if use_net:  # Forward of net returns the istft version\n",
    "                x_est = net(noisy.to(DEVICE), is_istft=True)\n",
    "                x_est_np = x_est.view(-1).detach().cpu().numpy()\n",
    "            else:\n",
    "                x_est_np = (\n",
    "                    torch.istft(\n",
    "                        torch.squeeze(noisy, 1),\n",
    "                        n_fft=N_FFT,\n",
    "                        hop_length=HOP_LENGTH,\n",
    "                        normalized=True,\n",
    "                    )\n",
    "                    .view(-1)\n",
    "                    .detach()\n",
    "                    .cpu()\n",
    "                    .numpy()\n",
    "                )\n",
    "            x_clean_np = (\n",
    "                torch.istft(\n",
    "                    torch.squeeze(clean, 1),\n",
    "                    n_fft=N_FFT,\n",
    "                    hop_length=HOP_LENGTH,\n",
    "                    normalized=True,\n",
    "                )\n",
    "                .view(-1)\n",
    "                .detach()\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "\n",
    "            metrics = AudioMetrics2(x_clean_np, x_est_np, 48000)\n",
    "\n",
    "            ref_wb = resample(x_clean_np, 48000, 16000)\n",
    "            deg_wb = resample(x_est_np, 48000, 16000)\n",
    "            pesq_wb = pesq(16000, ref_wb, deg_wb, \"wb\")\n",
    "\n",
    "            ref_nb = resample(x_clean_np, 48000, 8000)\n",
    "            deg_nb = resample(x_est_np, 48000, 8000)\n",
    "            pesq_nb = pesq(8000, ref_nb, deg_nb, \"nb\")\n",
    "\n",
    "            # print(new_scores)\n",
    "            # print(metrics.PESQ, metrics.STOI)\n",
    "\n",
    "            overall_metrics[0].append(pesq_wb)\n",
    "            overall_metrics[1].append(pesq_nb)\n",
    "            overall_metrics[2].append(metrics.SNR)\n",
    "            overall_metrics[3].append(metrics.SSNR)\n",
    "            overall_metrics[4].append(metrics.STOI)\n",
    "    print()\n",
    "    print(\"Sample metrics computed\")\n",
    "    results = {}\n",
    "    for i in range(5):\n",
    "        temp = {}\n",
    "        temp[\"Mean\"] = np.mean(overall_metrics[i])\n",
    "        temp[\"STD\"] = np.std(overall_metrics[i])\n",
    "        temp[\"Min\"] = min(overall_metrics[i])\n",
    "        temp[\"Max\"] = max(overall_metrics[i])\n",
    "        results[metric_names[i]] = temp\n",
    "    print(\"Averages computed\")\n",
    "    if use_net:\n",
    "        addon = \"(cleaned by model)\"\n",
    "    else:\n",
    "        addon = \"(pre denoising)\"\n",
    "    print(\"Metrics on test data\", addon)\n",
    "    for i in range(5):\n",
    "        print(\n",
    "            \"{} : {:.3f}+/-{:.3f}\".format(\n",
    "                metric_names[i], np.mean(overall_metrics[i]), np.std(overall_metrics[i])\n",
    "            )\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kYvWz_6jRZ3e"
   },
   "source": [
    "### Description of the training of epochs. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VukJTCGIZ8ZU"
   },
   "outputs": [],
   "source": [
    "def train_epoch(net, train_loader, loss_fn, optimizer):\n",
    "    net.train()\n",
    "    train_ep_loss = 0.0\n",
    "    counter = 0\n",
    "    for noisy_x, clean_x in train_loader:\n",
    "        noisy_x, clean_x = noisy_x.to(DEVICE), clean_x.to(DEVICE)\n",
    "\n",
    "        # zero  gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        pred_x = net(noisy_x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = loss_fn(noisy_x, pred_x, clean_x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_ep_loss += loss.item()\n",
    "        counter += 1\n",
    "\n",
    "    train_ep_loss /= counter\n",
    "\n",
    "    # clear cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return train_ep_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zYCJWaTMRgS3"
   },
   "source": [
    "### Description of the validation of epochs ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-JKrTMpPhw19"
   },
   "outputs": [],
   "source": [
    "def test_epoch(net, test_loader, loss_fn, use_net=True):\n",
    "    net.eval()\n",
    "    test_ep_loss = 0.0\n",
    "    counter = 0.0\n",
    "    \"\"\"\n",
    "    for noisy_x, clean_x in test_loader:\n",
    "        # get the output from the model\n",
    "        noisy_x, clean_x = noisy_x.to(DEVICE), clean_x.to(DEVICE)\n",
    "        pred_x = net(noisy_x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = loss_fn(noisy_x, pred_x, clean_x)\n",
    "        # Calc the metrics here\n",
    "        test_ep_loss += loss.item() \n",
    "        \n",
    "        counter += 1\n",
    "\n",
    "    test_ep_loss /= counter\n",
    "    \"\"\"\n",
    "\n",
    "    # print(\"Actual compute done...testing now\")\n",
    "\n",
    "    testmet = getMetricsonLoader(test_loader, net, use_net)\n",
    "\n",
    "    # clear cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return test_ep_loss, testmet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "879vq_uBRm_2"
   },
   "source": [
    "### To understand whether the network is being trained or not, we will output a train and test loss. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I4gdVmhRr1Qi"
   },
   "outputs": [],
   "source": [
    "def train(net, train_loader, test_loader, loss_fn, optimizer, scheduler, epochs):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for e in tqdm(range(epochs)):\n",
    "        # first evaluating for comparison\n",
    "\n",
    "        if e == 0 and training_type == \"Noise2Clean\":\n",
    "            print(\"Pre-training evaluation\")\n",
    "            # with torch.no_grad():\n",
    "            #    test_loss,testmet = test_epoch(net, test_loader, loss_fn,use_net=False)\n",
    "            # print(\"Had to load model.. checking if deets match\")\n",
    "            testmet = getMetricsonLoader(\n",
    "                test_loader, net, False\n",
    "            )  # again, modified cuz im loading\n",
    "            # test_losses.append(test_loss)\n",
    "            # print(\"Loss before training:{:.6f}\".format(test_loss))\n",
    "\n",
    "            with open(basepath + \"/results.txt\", \"w+\") as f:\n",
    "                f.write(\"Initial : \\n\")\n",
    "                f.write(str(testmet))\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "        train_loss = train_epoch(net, train_loader, loss_fn, optimizer)\n",
    "        test_loss = 0\n",
    "        scheduler.step()\n",
    "        print(\"Saving model....\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_loss, testmet = test_epoch(net, test_loader, loss_fn, use_net=True)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        # print(\"skipping testing cuz peak autism idk\")\n",
    "\n",
    "        with open(basepath + \"/results.txt\", \"a\") as f:\n",
    "            f.write(\"Epoch :\" + str(e + 1) + \"\\n\" + str(testmet))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        print(\"OPed to txt\")\n",
    "\n",
    "        torch.save(\n",
    "            net.state_dict(), basepath + \"/Weights/dc20_model_\" + str(e + 1) + \".pth\"\n",
    "        )\n",
    "        torch.save(\n",
    "            optimizer.state_dict(),\n",
    "            basepath + \"/Weights/dc20_opt_\" + str(e + 1) + \".pth\",\n",
    "        )\n",
    "\n",
    "        print(\"Models saved\")\n",
    "\n",
    "        # clear cache\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        # print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "        #              \"Loss: {:.6f}...\".format(train_loss),\n",
    "        #              \"Test Loss: {:.6f}\".format(test_loss))\n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HO3p2zrOcn_z"
   },
   "source": [
    "### 20 Layer DCUNet Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j8XCrVIg5i-K"
   },
   "outputs": [],
   "source": [
    "class DCUnet20(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Complex U-Net class of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_fft=64, hop_length=16):\n",
    "        super().__init__()\n",
    "\n",
    "        # for istft\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "        self.set_size(\n",
    "            model_complexity=int(45 // 1.414), input_channels=1, model_depth=20\n",
    "        )\n",
    "        self.encoders = []\n",
    "        self.model_length = 20 // 2\n",
    "\n",
    "        for i in range(self.model_length):\n",
    "            module = Encoder(\n",
    "                in_channels=self.enc_channels[i],\n",
    "                out_channels=self.enc_channels[i + 1],\n",
    "                filter_size=self.enc_kernel_sizes[i],\n",
    "                stride_size=self.enc_strides[i],\n",
    "                padding=self.enc_paddings[i],\n",
    "            )\n",
    "            self.add_module(\"encoder{}\".format(i), module)\n",
    "            self.encoders.append(module)\n",
    "\n",
    "        self.decoders = []\n",
    "\n",
    "        for i in range(self.model_length):\n",
    "            if i != self.model_length - 1:\n",
    "                module = Decoder(\n",
    "                    in_channels=self.dec_channels[i]\n",
    "                    + self.enc_channels[self.model_length - i],\n",
    "                    out_channels=self.dec_channels[i + 1],\n",
    "                    filter_size=self.dec_kernel_sizes[i],\n",
    "                    stride_size=self.dec_strides[i],\n",
    "                    padding=self.dec_paddings[i],\n",
    "                    output_padding=self.dec_output_padding[i],\n",
    "                )\n",
    "            else:\n",
    "                module = Decoder(\n",
    "                    in_channels=self.dec_channels[i]\n",
    "                    + self.enc_channels[self.model_length - i],\n",
    "                    out_channels=self.dec_channels[i + 1],\n",
    "                    filter_size=self.dec_kernel_sizes[i],\n",
    "                    stride_size=self.dec_strides[i],\n",
    "                    padding=self.dec_paddings[i],\n",
    "                    output_padding=self.dec_output_padding[i],\n",
    "                    last_layer=True,\n",
    "                )\n",
    "            self.add_module(\"decoder{}\".format(i), module)\n",
    "            self.decoders.append(module)\n",
    "\n",
    "    def forward(self, x, is_istft=True):\n",
    "        # print('x : ', x.shape)\n",
    "        orig_x = x\n",
    "        xs = []\n",
    "        for i, encoder in enumerate(self.encoders):\n",
    "            xs.append(x)\n",
    "            x = encoder(x)\n",
    "            # print('Encoder : ', x.shape)\n",
    "\n",
    "        p = x\n",
    "        for i, decoder in enumerate(self.decoders):\n",
    "            p = decoder(p)\n",
    "            if i == self.model_length - 1:\n",
    "                break\n",
    "            # print('Decoder : ', p.shape)\n",
    "            p = torch.cat([p, xs[self.model_length - 1 - i]], dim=1)\n",
    "\n",
    "        # u9 - the mask\n",
    "\n",
    "        mask = p\n",
    "\n",
    "        # print('mask : ', mask.shape)\n",
    "\n",
    "        output = mask * orig_x\n",
    "        output = torch.squeeze(output, 1)\n",
    "\n",
    "        if is_istft:\n",
    "            output = torch.istft(\n",
    "                output, n_fft=self.n_fft, hop_length=self.hop_length, normalized=True\n",
    "            )\n",
    "\n",
    "        return output\n",
    "\n",
    "    def set_size(self, model_complexity, model_depth=20, input_channels=1):\n",
    "        if model_depth == 20:\n",
    "            self.enc_channels = [\n",
    "                input_channels,\n",
    "                model_complexity,\n",
    "                model_complexity,\n",
    "                model_complexity * 2,\n",
    "                model_complexity * 2,\n",
    "                model_complexity * 2,\n",
    "                model_complexity * 2,\n",
    "                model_complexity * 2,\n",
    "                model_complexity * 2,\n",
    "                model_complexity * 2,\n",
    "                128,\n",
    "            ]\n",
    "\n",
    "            self.enc_kernel_sizes = [\n",
    "                (7, 1),\n",
    "                (1, 7),\n",
    "                (6, 4),\n",
    "                (7, 5),\n",
    "                (5, 3),\n",
    "                (5, 3),\n",
    "                (5, 3),\n",
    "                (5, 3),\n",
    "                (5, 3),\n",
    "                (5, 3),\n",
    "            ]\n",
    "\n",
    "            self.enc_strides = [\n",
    "                (1, 1),\n",
    "                (1, 1),\n",
    "                (2, 2),\n",
    "                (2, 1),\n",
    "                (2, 2),\n",
    "                (2, 1),\n",
    "                (2, 2),\n",
    "                (2, 1),\n",
    "                (2, 2),\n",
    "                (2, 1),\n",
    "            ]\n",
    "\n",
    "            self.enc_paddings = [\n",
    "                (3, 0),\n",
    "                (0, 3),\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "            ]\n",
    "\n",
    "            self.dec_channels = [\n",
    "                0,\n",
    "                model_complexity * 2,\n",
    "                model_complexity * 2,\n",
    "                model_complexity * 2,\n",
    "                model_complexity * 2,\n",
    "                model_complexity * 2,\n",
    "                model_complexity * 2,\n",
    "                model_complexity * 2,\n",
    "                model_complexity,\n",
    "                model_complexity,\n",
    "                1,\n",
    "            ]\n",
    "\n",
    "            self.dec_kernel_sizes = [\n",
    "                (6, 3),\n",
    "                (6, 3),\n",
    "                (6, 3),\n",
    "                (6, 4),\n",
    "                (6, 3),\n",
    "                (6, 4),\n",
    "                (8, 5),\n",
    "                (7, 5),\n",
    "                (1, 7),\n",
    "                (7, 1),\n",
    "            ]\n",
    "\n",
    "            self.dec_strides = [\n",
    "                (2, 1),  #\n",
    "                (2, 2),  #\n",
    "                (2, 1),  #\n",
    "                (2, 2),  #\n",
    "                (2, 1),  #\n",
    "                (2, 2),  #\n",
    "                (2, 1),  #\n",
    "                (2, 2),  #\n",
    "                (1, 1),\n",
    "                (1, 1),\n",
    "            ]\n",
    "\n",
    "            self.dec_paddings = [\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (0, 3),\n",
    "                (3, 0),\n",
    "            ]\n",
    "\n",
    "            self.dec_output_padding = [\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "                (0, 0),\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model depth : {}\".format(model_depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G6bIE9iOj8pq"
   },
   "source": [
    "## Training New Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QyBc1awQkI-D"
   },
   "outputs": [],
   "source": [
    "# # clear cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "dcunet20 = DCUnet20(N_FFT, HOP_LENGTH).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(dcunet20.parameters())\n",
    "loss_fn = wsdr_fn\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify paths and uncomment to resume training from a given point\n",
    "# model_checkpoint = torch.load(path_to_model)\n",
    "# opt_checkpoint = torch.load(path_to_opt)\n",
    "# dcunet20.load_state_dict(model_checkpoint)\n",
    "# optimizer.load_state_dict(opt_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ppXkJUsY55vI"
   },
   "outputs": [],
   "source": [
    "train_losses, test_losses = train(\n",
    "    dcunet20, train_loader, test_loader, loss_fn, optimizer, scheduler, 4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pretrained weights to run denoising inference ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the model weight .pth file ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_path = \"Pretrained_Weights/Noise2Noise/white.pth\"\n",
    "\n",
    "dcunet20 = DCUnet20(N_FFT, HOP_LENGTH).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(dcunet20.parameters())\n",
    "\n",
    "checkpoint = torch.load(model_weights_path, map_location=torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the testing audio folders for inference ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_noisy_files = sorted(list(Path(\"Samples/Sample_Test_Input\").rglob(\"*.wav\")))\n",
    "test_clean_files = sorted(list(Path(\"Samples/Sample_Test_Target\").rglob(\"*.wav\")))\n",
    "\n",
    "test_dataset = SpeechDataset(test_noisy_files, test_clean_files, N_FFT, HOP_LENGTH)\n",
    "\n",
    "# For testing purpose\n",
    "test_loader_single_unshuffled = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcunet20.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter the index of the file in the Test Set folder to Denoise and evaluate metrics waveforms (Indexing starts from 0) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcunet20.eval()\n",
    "test_loader_single_unshuffled_iter = iter(test_loader_single_unshuffled)\n",
    "\n",
    "x_n, x_c = next(test_loader_single_unshuffled_iter)\n",
    "for _ in range(index):\n",
    "    x_n, x_c = next(test_loader_single_unshuffled_iter)\n",
    "\n",
    "x_est = dcunet20(x_n, is_istft=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_est_np = x_est[0].view(-1).detach().cpu().numpy()\n",
    "x_c_np = (\n",
    "    torch.istft(\n",
    "        torch.squeeze(x_c[0], 1), n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True\n",
    "    )\n",
    "    .view(-1)\n",
    "    .detach()\n",
    "    .cpu()\n",
    "    .numpy()\n",
    ")\n",
    "x_n_np = (\n",
    "    torch.istft(\n",
    "        torch.squeeze(x_n[0], 1), n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True\n",
    "    )\n",
    "    .view(-1)\n",
    "    .detach()\n",
    "    .cpu()\n",
    "    .numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = AudioMetrics(x_c_np, x_est_np, SAMPLE_RATE)\n",
    "print(metrics.display())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of denoising the audio in /Samples folder ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noisy audio waveform ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_n_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model denoised audio waveform ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_est_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### True clean audio waveform ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_c_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Recently Denoised Speech Files ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_addition_utils.save_audio_file(\n",
    "    np_array=x_est_np,\n",
    "    file_path=Path(\"Samples/denoised.wav\"),\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    bit_precision=16,\n",
    ")\n",
    "noise_addition_utils.save_audio_file(\n",
    "    np_array=x_c_np,\n",
    "    file_path=Path(\"Samples/clean.wav\"),\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    bit_precision=16,\n",
    ")\n",
    "noise_addition_utils.save_audio_file(\n",
    "    np_array=x_n_np,\n",
    "    file_path=Path(\"Samples/noisy.wav\"),\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    bit_precision=16,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "dcunet.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
